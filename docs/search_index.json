[["index.html", "Desagregación de datos en encuestas de hogares: metodologías bayesianas para modelos de estimación en áreas pequeñas Agenda", " Desagregación de datos en encuestas de hogares: metodologías bayesianas para modelos de estimación en áreas pequeñas Andrés Gutiérrez1, Stalyn Guerrero2 2023-05-03 Agenda Experto Regional en Estadísticas Sociales - Comisión Económica para América Latina y el Caribe (CEPAL) - andres.gutierrez@cepal.org↩︎ Consultor - Comisión Económica para América Latina y el Caribe (CEPAL), guerrerostalyn@gmail.com↩︎ "],["material-del-curso.html", "Material del curso", " Material del curso En el siguiente enlace encontrará material bibliográfico complementario (Libros, presentaciones, casos de estudio y manuales de instalación) Descargar En el siguiente enlace encontrará las rutinas de R desarrolladas para el taller. Descargar "],["día-1---sesión-1--no-dejar-a-nadie-atrás---ods-y-la-agenda-2030.html", "Capítulo 1 Día 1 - Sesión 1- No dejar a nadie atrás - ODS y la Agenda 2030", " Capítulo 1 Día 1 - Sesión 1- No dejar a nadie atrás - ODS y la Agenda 2030 Ver presentación "],["día-1---sesión-2--censo-e-información-satelital.html", "Capítulo 2 Día 1 - Sesión 2- Censo e información satelital ", " Capítulo 2 Día 1 - Sesión 2- Censo e información satelital "],["uso-de-imágenes-satelitales-y-sae.html", "2.1 Uso de imágenes satelitales y SAE", " 2.1 Uso de imágenes satelitales y SAE Uno de los artículo pioneros de estimación de áreas pequeñas fue el artículo de Singh, R, et. al. (2002) el cual abordó la estimación del rendimiento de cultivos para los tehsil (unidad subadministrativa) del distriyo Rohtak district en Haryana (India). Las imágenes raster representan el mundo mediante un conjunto de celdas contiguas igualmente espaciadas conocidas como pixeles, estas imágenes tienen información como un sistema de información geográfico, Un sistema de referencia de coordenadas. Las imágenes almacenan un identificador, un valor en cada pixel (o un vector con diferentes valores) y cada celda tiene asociada una escala de colores. Las imágenes pueden obtenerse crudas y procesadas, estas primeras contienen solamente las capas de colores, las segundas contienen también valores que han sido procesados en cada celda (índices de vegetación, intensidad lumínica, tipo de vegetación). La información cruda puede utilizarse para entrenar características que se desean entrenar (carreteras, tipo de cultivo, bosque / no bosque), afortunadamente en Google Earth Engine encontramos muchos indicadores procesadas asociadas a un pixel. Estos indicadores pueden agregarse a nivel de un área geográfica. 2.1.1 Fuentes de datos de imágenes satelitales Algunas de las principales fuentes de imágenes satelitales son: http://earthexplorer.usgs.gov/ https://lpdaacsvc.cr.usgs.gov/appeears/ https://search.earthdata.nasa.gov/search https://scihub.copernicus.eu/ https://aws.amazon.com/public-data-sets/landsat/ Sin embargo la mayor parte de estas fuentes están centralizadas en Google Earth Engine que permite buscar fuentes de datos provenientes de imágenes satelitales. GEE se puede manejar por medio de APIS en diferentes lenguajes de programación: Javascript (por defecto), Python y R (paquete rgee). "],["google-earth-eninge.html", "2.2 Google Earth Eninge", " 2.2 Google Earth Eninge Crear una cuenta en link, una vez que se ingrese a la cuenta puede buscarse los conjuntos de datos de interés: Una vez se busque el conjunto de datos se puede abrir un editor de código brindado por google en Javascript. Copiar y pegar la sintaxis que brinda el buscador de conjunto de datos para visualizar la imagen raster y disponer de sentencias que Permitan la obtención del conjunto de datos de interés posteriormente en R "],["instalación-de-rgee.html", "2.3 Instalación de rgee", " 2.3 Instalación de rgee Descargar e instalar anaconda o conda. (https://www.anaconda.com/products/individual) Abrir Anaconda prompt y configurar ambiente de trabajo (ambiente python rgee_py) con las siguientes sentencias: conda create -n rgee_py python=3.9 activate rgee_py pip install google-api-python-client pip install earthengine-api pip install numpy Listar los ambientes de Python disponibles en anaconda prompt conda env list Una vez identificado la ruta del ambiente ambiente rgee_py definirla en R (no se debe olvidar cambiar \\ por /). Instalar reticulate y rgee, cargar paquetes para procesamiento espacial y configurar el ambiente de trabajo como sigue: library(reticulate) # Conexión con Python library(rgee) # Conexión con Google Earth Engine library(sf) # Paquete para manejar datos geográficos library(dplyr) # Paquete para procesamiento de datos library(magrittr) rgee_environment_dir = &quot;C://Users//sguerrero//Anaconda3//envs//rgee_py//python.exe&quot; # Configurar python (Algunas veces no es detectado y se debe reiniciar R) reticulate::use_python(rgee_environment_dir, required=T) rgee::ee_install_set_pyenv(py_path = rgee_environment_dir, py_env = &quot;rgee_py&quot;) Sys.setenv(RETICULATE_PYTHON = rgee_environment_dir) Sys.setenv(EARTHENGINE_PYTHON = rgee_environment_dir) Una vez configurado el ambiente puede iniciarlizarse una sesión de Google Earth Engine como sigue: rgee::ee_Initialize(drive = T) Notas: Se debe inicializar cada sesión con el comando rgee::ee_Initialize(drive = T). Los comandos de javascript que invoquen métodos con “.” se sustituyen por signo peso ($), por ejemplo: ee.ImageCollection().filterDate() # Javascript ee$ImageCollection()$filterDate() # R 2.3.1 Descargar información satelital Paso 1: disponer de los shapefile shape &lt;- read_sf(&quot;Recursos/Día1/Sesion2/Shape/DOM.shp&quot;) plot(shape[&quot;geometry&quot;]) Paso 2: Seleccionar el archivo de imágenes que desea procesar, para nuestro ejemplo luces nocturnas. luces &lt;- ee$ImageCollection(&quot;NOAA/DMSP-OLS/NIGHTTIME_LIGHTS&quot;) %&gt;% ee$ImageCollection$filterDate(&quot;2013-01-01&quot;, &quot;2014-01-01&quot;) %&gt;% ee$ImageCollection$map(function(x) x$select(&quot;stable_lights&quot;)) %&gt;% ee$ImageCollection$toBands() Paso 3: Descargar la información shape_luces &lt;- map(unique(shape$dam), ~tryCatch(ee_extract( x = luces, y = shape[&quot;dam&quot;] %&gt;% filter(dam == .x), ee$Reducer$mean(), sf = FALSE ) %&gt;% mutate(dam = .x), error = function(e)data.frame(dam = .x))) shape_luces %&lt;&gt;% bind_rows() tba(shape_luces, cap = &quot;Promedio de luces nocturnasa&quot;) Repetir la rutina para: Tipo de suelo: crops-coverfraction (Porcentaje de cubrimiento cultivos) y urban-coverfraction (Porcentaje de cobertura urbana) disponibles en https://developers.google.com/earth-engine/datasets/catalog/COGTMNICUS_Landcover_100m_Proba-V-C3_Global#description Tiempo de viaje al hospital o clínica más cercana (accessibility) y tiempo de viaje al hospital o clínica más cercana utilizando transporte no motorizado (accessibility_walking_only) información disponible en https://develoGTMs.google.com/earth-engine/datasets/catalog/Oxford_MAP_accessibility_to_healthcare_2019 Modificación humana, donde se consideran los asentamiento humano, la agricultura, el transporte, la minería y producción de energía e infraestructura eléctrica. En el siguiente link encuentra la información satelital https://develoGTMs.google.com/earth-engine/datasets/catalog/CSP_HM_GlobalHumanModification#description Paso 4 consolidar la información. dam luces_nocturnas cubrimiento_cultivo cubrimiento_urbano modificacion_humana accesibilidad_hospitales accesibilidad_hosp_caminado 02 97.27735 107.10243 100.28394 111.01645 112.25437 109.66652 03 94.36142 97.71126 96.32667 96.29454 100.55525 105.35470 04 96.53042 95.73335 96.55354 98.67362 102.77860 103.78255 05 91.61138 98.91270 93.17994 95.16236 97.72809 95.59357 01 97.65958 86.79795 106.90397 82.34083 89.53466 87.11064 06 100.52567 107.21708 99.96448 104.95498 94.75524 95.69942 08 94.54495 103.38473 93.84241 102.73169 98.74441 101.96155 09 96.82438 91.41961 97.18788 92.96846 91.28395 90.46557 30 95.68487 91.78986 93.93370 96.83847 98.54392 98.93543 10 92.18900 98.33939 94.53840 97.45111 108.54346 107.34596 Los resultados se muestran en los siguientes mapas 2.3.2 Luces nocturnas 2.3.3 Cubrimiento cultivos 2.3.4 Cubrimiento urbanos 2.3.5 Modificación humana 2.3.6 Tiempo promedio al hospital 2.3.7 Tiempo promedio al hospital en vehiculo no motorizado "],["censos-de-población-y-vivienda.html", "2.4 Censos de población y vivienda", " 2.4 Censos de población y vivienda Es necesario definir las variables del país con los que se desea trabajar. De acuerdo a esto, como primer paso se debe tener acceso al censo del país, para ello puede acceder desde el siguiente enlace https://redatam.org/en/microdata en el cual dispondrá de un archivo .zip con los microdatos del país. Ahora bien, para leer el conjunto de datos, es necesario emplear la función redatam.open de la librería redatam, la cual depende directamente del diccionario censal del software REDATAM, este es un archivo con extensión dicx y que debe encontrarse en la carpeta sobre los datos que se están leyendo. Así, es como se crea un objeto dentro de R que hace la respectiva unión del diccionario con los microdatos de la base de datos censal. La siguiente sintaxis muestra la lectura del diccionario en R y los cálculos iniciales # https://redatamr.ideasybits.com/ library(redatam) RepDoma &lt;- redatam.open(&quot;cpv2010dom-cde.dicX&quot;) CONTEOS &lt;- redatam.query(RepDoma, &quot;freq PROVIC.IDPROVI by VIVIENDA.ZONA by PERSONA.P27 by PERSONA.P29 by PERSONA.ANEST&quot;, tot.omit = FALSE) Después de realizar algunas validaciones se estandarizan las variables como muestra el siguiente código. CONTEOS &lt;- readRDS(file = &quot;Recursos/Día1/Sesion2/Data/CONTEOS.RDS&quot;) # Eliminando totales de la tabla CONTEOS2 &lt;- CONTEOS %&gt;% filter_at(vars(matches(&quot;_label&quot;)),all_vars(. != &quot;__tot__&quot;)) censo_mrp &lt;- CONTEOS2 %&gt;% transmute(dam = str_pad( string = IDPROVI1_value, width = 2, pad = &quot;0&quot; ), area = case_when(ZONA2_value == 1 ~ &quot;1&quot;, # 1 = Urbana TRUE ~ &quot;0&quot;), sexo = as.character(P273_value), edad = case_when( P294_value %in% 0:14 ~ &quot;1&quot;, # 0 a 14 P294_value %in% 15:29 ~ &quot;2&quot;, # 15 a 29 P294_value %in% 30:44 ~ &quot;3&quot;, # 30 a 44 P294_value %in% 45:64 ~ &quot;4&quot;, # 45 a 64 TRUE ~ &quot;5&quot;), # 65 o mas anoest = case_when( P294_value &lt; 5| is.na(ANEST5_value) ~ &quot;98&quot;, # No aplica ANEST5_value == 99 ~ &quot;99&quot;, #NS/NR ANEST5_value %in% 0 ~ &quot;1&quot;, # Sin educacion ANEST5_value %in% c(1:6) ~ &quot;2&quot;, # 1-6 ANEST5_value %in% c(7:12) ~ &quot;3&quot;, # 7-12 ANEST5_value &gt; 12 ~ &quot;4&quot; , # 12 o mas TRUE ~ &quot;Error&quot; ), value) %&gt;% group_by(dam, area, sexo, edad,anoest) %&gt;% summarise(n = sum(value), .groups = &quot;drop&quot;) A partir de la base estandarizada es posible construir algunas covariables para la dam. censo_mrp &lt;- readRDS(&quot;Recursos/Día1/Sesion2/Data/censo_mrp_dam.rds&quot;) tasa_censo &lt;- model.matrix(dam ~ -1 +., data = censo_mrp %&gt;% select(-n)) %&gt;% data.frame() %&gt;% mutate(dam = censo_mrp$dam, n = censo_mrp$n) %&gt;% group_by(dam) %&gt;% summarise_all(~weighted.mean(x = .,w = n)) %&gt;% select(-area0, -anoest98,-anoest98,-n) tba(tasa_censo) dam area1 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 anoest99 01 1.0000 0.5224 0.2781 0.2117 0.1808 0.0725 0.2000 0.3680 0.2286 0.0193 02 0.7718 0.4733 0.2773 0.1831 0.1531 0.0634 0.3350 0.2771 0.0540 0.0065 03 0.7128 0.4804 0.2643 0.1572 0.1489 0.0718 0.3470 0.2542 0.0493 0.0055 04 0.8345 0.4826 0.2756 0.1738 0.1454 0.0638 0.3169 0.2948 0.0827 0.0082 05 0.5977 0.4849 0.2509 0.1752 0.1748 0.0899 0.3471 0.3252 0.0625 0.0052 06 0.6626 0.4909 0.2671 0.1988 0.1736 0.0786 0.3140 0.3383 0.1038 0.0087 07 0.4828 0.4768 0.2380 0.1431 0.1527 0.0788 0.3287 0.1949 0.0315 0.0037 08 0.5144 0.4610 0.2648 0.1728 0.1625 0.0845 0.3822 0.2730 0.0440 0.0035 09 0.4535 0.4889 0.2733 0.2084 0.1694 0.0764 0.3125 0.3699 0.0840 0.0078 10 0.7996 0.4865 0.2658 0.1648 0.1418 0.0665 0.3214 0.2603 0.0485 0.0052 11 0.7784 0.4766 0.3001 0.2393 0.1250 0.0430 0.2986 0.3722 0.0650 0.0076 12 0.9425 0.5059 0.2821 0.2061 0.1466 0.0515 0.2810 0.3955 0.0794 0.0080 13 0.4696 0.4885 0.2762 0.2049 0.1606 0.0719 0.3142 0.3582 0.0781 0.0070 14 0.5252 0.4855 0.2700 0.1929 0.1729 0.0731 0.3258 0.3517 0.0765 0.0079 15 0.5312 0.4718 0.2685 0.1984 0.1764 0.0798 0.3103 0.3318 0.0519 0.0048 16 0.6441 0.4651 0.2738 0.1932 0.1334 0.0459 0.2589 0.2403 0.0316 0.0028 17 0.6810 0.5047 0.2665 0.1984 0.1549 0.0646 0.3707 0.3252 0.0460 0.0052 18 0.5839 0.4895 0.2699 0.2126 0.1689 0.0703 0.2875 0.3940 0.0727 0.0067 19 0.2891 0.4902 0.2568 0.2001 0.1839 0.0954 0.3037 0.3483 0.1097 0.0078 20 0.4354 0.4837 0.2811 0.2007 0.1554 0.0628 0.3080 0.3691 0.0595 0.0059 21 0.5186 0.4996 0.2807 0.1956 0.1451 0.0507 0.2931 0.3782 0.0783 0.0082 22 0.6009 0.4725 0.2559 0.1698 0.1695 0.0891 0.3482 0.2674 0.0642 0.0075 23 0.8408 0.5063 0.2861 0.1974 0.1522 0.0572 0.2840 0.4022 0.0817 0.0077 24 0.5628 0.4827 0.2754 0.1871 0.1662 0.0743 0.3198 0.3320 0.1000 0.0089 25 0.7561 0.5017 0.2795 0.2221 0.1583 0.0649 0.2843 0.3727 0.1106 0.0102 26 0.4795 0.4776 0.2632 0.1821 0.1770 0.1038 0.3340 0.3156 0.0767 0.0074 27 0.7917 0.4785 0.2810 0.2098 0.1501 0.0681 0.3121 0.3151 0.0817 0.0074 28 0.6797 0.4949 0.2690 0.2028 0.1597 0.0655 0.2883 0.3819 0.0855 0.0101 29 0.4939 0.4836 0.2656 0.1718 0.1537 0.0756 0.3438 0.3188 0.0526 0.0056 30 0.7412 0.4902 0.2722 0.1855 0.1624 0.0775 0.3308 0.3338 0.0696 0.0070 31 0.6292 0.4587 0.2556 0.2037 0.1761 0.0848 0.3785 0.2681 0.0602 0.0050 32 0.8780 0.5098 0.2870 0.2126 0.1502 0.0447 0.2409 0.4005 0.1291 0.0131 Es posible construir a partir de una variable del censo, haciendo que el proceso se hace más corto, para este caso es empleada la variable VIVIENDA.V05, agregada por dam En el primer bloque que código usando la función redatam.query() se realiza el conteo de viviendas que tienen el piso de tierra. Seguido de esto, eliminamos los registros que no son de interés, por ejemplo, el total en la dam o total nacional, los cuales se identifican dentro de la base con la etiqueta __tot__. El siguiente paso es contar el número de viviendas por dam que tienen piso de tierra en el censo (Pobx) y el total de viviendas que respondieron a la pregunta (PobT), para finalmente realizar el cociente de estas dos preguntas. CONTEOS &lt;- redatam.query(RepDoma, &quot;freq PROVIC.IDPROVI by VIVIENDA.V05&quot;, tot.omit = FALSE) PISO &lt;- CONTEOS %&gt;% filter_at(vars(matches(&quot;_label&quot;)), all_vars(!. %in% c(&quot;__tot__&quot;,&quot;__mv__&quot;) )) tasa_piso &lt;- PISO %&gt;% mutate(Pobx = ifelse(V052_value %in% c(7), value, 0), PobT = value) %&gt;% group_by( depto = str_pad( string = IDPROVI1_value, width = 2, pad = &quot;0&quot; ) ) %&gt;% summarise(PobT = sum(PobT), Pobx = sum(Pobx)) %&gt;% transmute(depto, piso_tierra = Pobx/PobT) La tabla resultante se muestra a continuación. dam piso_tierra 02 0.1038 03 0.1435 04 0.0828 05 0.0593 01 0.0033 06 0.0191 08 0.0582 09 0.0365 30 0.0564 10 0.1133 11 0.0089 07 0.3020 12 0.0120 13 0.0214 14 0.0252 28 0.0197 15 0.0926 29 0.0523 16 0.2136 17 0.0257 18 0.0422 24 0.0287 19 0.0155 20 0.0211 21 0.0323 31 0.0734 22 0.1748 23 0.0221 26 0.0357 25 0.0170 32 0.0152 27 0.0620 El proceso se repite con otras preguntas del censo hasta consolidar la tabla siguiente. predictors_censo_dam &lt;- readRDS(&quot;Recursos/Día1/Sesion2/Data/predictors_censo_dam.rds&quot;) tba(predictors_censo_dam) dam area1 sexo2 edad2 edad3 edad4 edad5 anoest2 anoest3 anoest4 anoest99 tiene_sanitario tiene_acueducto tiene_gas eliminar_basura tiene_internet piso_tierra material_paredes material_techo rezago_escolar alfabeta hacinamiento tasa_desocupacion 02 0.7718 0.4733 0.2773 0.1831 0.1531 0.0634 0.3350 0.2771 0.0540 0.0065 0.1587 0.7424 0.3233 0.1962 0.9594 0.1038 0.1229 0.0489 0.1050 0.7512 0.3066 0.0016 03 0.7128 0.4804 0.2643 0.1572 0.1489 0.0718 0.3470 0.2542 0.0493 0.0055 0.2531 0.6512 0.4074 0.4032 0.9705 0.1435 0.2050 0.0935 0.1047 0.7433 0.2683 0.0008 04 0.8345 0.4826 0.2756 0.1738 0.1454 0.0638 0.3169 0.2948 0.0827 0.0082 0.1426 0.7577 0.3386 0.2966 0.9441 0.0828 0.1261 0.0559 0.1652 0.8083 0.3060 0.0013 05 0.5977 0.4849 0.2509 0.1752 0.1748 0.0899 0.3471 0.3252 0.0625 0.0052 0.0624 0.8704 0.2923 0.3864 0.9466 0.0593 0.0901 0.0339 0.1133 0.8275 0.1455 0.0006 01 1.0000 0.5224 0.2781 0.2117 0.1808 0.0725 0.2000 0.3680 0.2286 0.0193 0.0119 0.7946 0.0673 0.0810 0.6678 0.0033 0.0109 0.0111 0.3694 0.9247 0.1962 0.0066 06 0.6626 0.4909 0.2671 0.1988 0.1736 0.0786 0.3140 0.3383 0.1038 0.0087 0.0656 0.5465 0.2116 0.3791 0.9119 0.0191 0.0955 0.0149 0.1718 0.8580 0.1501 0.0021 08 0.5144 0.4610 0.2648 0.1728 0.1625 0.0845 0.3822 0.2730 0.0440 0.0035 0.2241 0.5158 0.4239 0.4996 0.9655 0.0582 0.2128 0.0240 0.0816 0.7910 0.3178 0.0008 09 0.4535 0.4889 0.2733 0.2084 0.1694 0.0764 0.3125 0.3699 0.0840 0.0078 0.0496 0.6379 0.1732 0.3440 0.9114 0.0365 0.0806 0.0145 0.1391 0.8654 0.1709 0.0013 30 0.7412 0.4902 0.2722 0.1855 0.1624 0.0775 0.3308 0.3338 0.0696 0.0070 0.1346 0.4029 0.2879 0.3509 0.9449 0.0564 0.1727 0.0209 0.1269 0.8358 0.2080 0.0007 10 0.7996 0.4865 0.2658 0.1648 0.1418 0.0665 0.3214 0.2603 0.0485 0.0052 0.1983 0.7116 0.4010 0.3478 0.9788 0.1133 0.1710 0.1151 0.1016 0.7355 0.3088 0.0004 11 0.7784 0.4766 0.3001 0.2393 0.1250 0.0430 0.2986 0.3722 0.0650 0.0076 0.0499 0.1475 0.1943 0.2442 0.9006 0.0089 0.0871 0.0182 0.1158 0.8615 0.3985 0.0028 07 0.4828 0.4768 0.2380 0.1431 0.1527 0.0788 0.3287 0.1949 0.0315 0.0037 0.2423 0.5225 0.6287 0.5988 0.9826 0.3020 0.2668 0.0577 0.0710 0.6320 0.2933 0.0005 12 0.9425 0.5059 0.2821 0.2061 0.1466 0.0515 0.2810 0.3955 0.0794 0.0080 0.0482 0.7438 0.1351 0.1958 0.8864 0.0120 0.0710 0.0404 0.1455 0.8905 0.3271 0.0021 13 0.4696 0.4885 0.2762 0.2049 0.1606 0.0719 0.3142 0.3582 0.0781 0.0070 0.0482 0.4743 0.1600 0.2868 0.9109 0.0214 0.0301 0.0215 0.1327 0.8496 0.1545 0.0024 14 0.5252 0.4855 0.2700 0.1929 0.1729 0.0731 0.3258 0.3517 0.0765 0.0079 0.0914 0.6197 0.2136 0.4319 0.9307 0.0252 0.1265 0.0256 0.1335 0.8534 0.1747 0.0011 28 0.6797 0.4949 0.2690 0.2028 0.1597 0.0655 0.2883 0.3819 0.0855 0.0101 0.0385 0.7019 0.1433 0.1773 0.8864 0.0197 0.0217 0.0151 0.1560 0.8700 0.1483 0.0015 15 0.5312 0.4718 0.2685 0.1984 0.1764 0.0798 0.3103 0.3318 0.0519 0.0048 0.0963 0.8061 0.3138 0.4871 0.9562 0.0926 0.0724 0.0409 0.0894 0.7713 0.1734 0.0007 29 0.4939 0.4836 0.2656 0.1718 0.1537 0.0756 0.3438 0.3188 0.0526 0.0056 0.1584 0.3967 0.3666 0.5787 0.9690 0.0523 0.2579 0.0166 0.1000 0.8193 0.1801 0.0015 16 0.6441 0.4651 0.2738 0.1932 0.1334 0.0459 0.2589 0.2403 0.0316 0.0028 0.2475 0.6564 0.4743 0.4722 0.9689 0.2136 0.1851 0.1202 0.0647 0.6170 0.3718 0.0002 17 0.6810 0.5047 0.2665 0.1984 0.1549 0.0646 0.3707 0.3252 0.0460 0.0052 0.0597 0.6812 0.2013 0.2577 0.9238 0.0257 0.1211 0.0286 0.0852 0.8494 0.2445 0.0012 18 0.5839 0.4895 0.2699 0.2126 0.1689 0.0703 0.2875 0.3940 0.0727 0.0067 0.0508 0.6276 0.2385 0.2723 0.8766 0.0422 0.1088 0.0321 0.1231 0.8535 0.1761 0.0029 24 0.5628 0.4827 0.2754 0.1871 0.1662 0.0743 0.3198 0.3320 0.1000 0.0089 0.0888 0.5112 0.2304 0.4438 0.9395 0.0287 0.1069 0.0106 0.1725 0.8535 0.1312 0.0014 19 0.2891 0.4902 0.2568 0.2001 0.1839 0.0954 0.3037 0.3483 0.1097 0.0078 0.0336 0.3167 0.2653 0.5648 0.9315 0.0155 0.0943 0.0099 0.1731 0.8522 0.1199 0.0006 20 0.4354 0.4837 0.2811 0.2007 0.1554 0.0628 0.3080 0.3691 0.0595 0.0059 0.1084 0.5409 0.2254 0.4523 0.9463 0.0211 0.1233 0.0342 0.1070 0.8418 0.2265 0.0009 21 0.5186 0.4996 0.2807 0.1956 0.1451 0.0507 0.2931 0.3782 0.0783 0.0082 0.0598 0.6674 0.1860 0.3141 0.9105 0.0323 0.0689 0.0185 0.1483 0.8755 0.2328 0.0047 31 0.6292 0.4587 0.2556 0.2037 0.1761 0.0848 0.3785 0.2681 0.0602 0.0050 0.0938 0.8112 0.3792 0.3025 0.9654 0.0734 0.1714 0.0277 0.1021 0.7747 0.2180 0.0005 22 0.6009 0.4725 0.2559 0.1698 0.1695 0.0891 0.3482 0.2674 0.0642 0.0075 0.1496 0.7350 0.3965 0.3638 0.9565 0.1748 0.2050 0.0633 0.1236 0.7571 0.2152 0.0019 23 0.8408 0.5063 0.2861 0.1974 0.1522 0.0572 0.2840 0.4022 0.0817 0.0077 0.1011 0.6079 0.1780 0.2227 0.8981 0.0221 0.0610 0.0161 0.1481 0.8904 0.2771 0.0023 26 0.4795 0.4776 0.2632 0.1821 0.1770 0.1038 0.3340 0.3156 0.0767 0.0074 0.0542 0.7367 0.2924 0.4135 0.9223 0.0357 0.0765 0.0414 0.1274 0.8042 0.1157 0.0005 25 0.7561 0.5017 0.2795 0.2221 0.1583 0.0649 0.2843 0.3727 0.1106 0.0102 0.0246 0.8397 0.1335 0.1783 0.8308 0.0170 0.0179 0.0176 0.1831 0.8801 0.1668 0.0062 32 0.8780 0.5098 0.2870 0.2126 0.1502 0.0447 0.2409 0.4005 0.1291 0.0131 0.0348 0.6973 0.0795 0.2003 0.8280 0.0152 0.0365 0.0146 0.2311 0.9110 0.2110 0.0188 27 0.7917 0.4785 0.2810 0.2098 0.1501 0.0681 0.3121 0.3151 0.0817 0.0074 0.0623 0.8198 0.2460 0.1664 0.9299 0.0620 0.0491 0.0277 0.1404 0.7995 0.1739 0.0010 2.4.1 Mapas de las variables con información censal. temp2 &lt;- inner_join(shape[&quot;dam&quot;], predictors_censo_dam) for(ii in names(predictors_censo_dam[,-1])){ plot( temp2[ii], key.pos = 4, breaks = quantile(temp2[[ii]])) } "],["día-2---sesión-1--fundamentos-de-la-inferencia-bayesiana-en-r-y-stan.html", "Capítulo 3 Día 2 - Sesión 1- Fundamentos de la inferencia Bayesiana en R y STAN", " Capítulo 3 Día 2 - Sesión 1- Fundamentos de la inferencia Bayesiana en R y STAN El proyecto Manhattan y la estimación desagregada con encuestas de hogares "],["día-2---sesión-1--modelos-sintéticos-simples.html", "Capítulo 4 Día 2 - Sesión 1- Modelos sintéticos simples ", " Capítulo 4 Día 2 - Sesión 1- Modelos sintéticos simples "],["regla-de-bayes.html", "4.1 Regla de Bayes", " 4.1 Regla de Bayes En términos de inferencia para \\(\\boldsymbol{\\theta}\\), es necesario encontrar la distribución de los parámetros condicionada a la observación de los datos. Para este fin, es necesario definir la distribución conjunta de la variable de interés con el vector de parámetros. \\[ p(\\boldsymbol{\\theta},\\mathbf{Y})=p(\\boldsymbol{\\theta})p(\\mathbf{Y} \\mid \\boldsymbol{\\theta}) \\] La distribución \\(p(\\boldsymbol{\\theta})\\) se le conoce con el nombre de distribución previa. El término \\(p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})\\) es la distribución de muestreo, verosimilitud o distribución de los datos. La distribución del vector de parámetros condicionada a los datos observados está dada por \\[ p(\\boldsymbol{\\theta} \\mid \\mathbf{Y})=\\frac{p(\\boldsymbol{\\theta},\\mathbf{Y})}{p(\\mathbf{Y})}=\\frac{p(\\boldsymbol{\\theta})p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})}{p(\\mathbf{Y})} \\] A la distribución \\(p(\\boldsymbol{\\theta} \\mid \\mathbf{Y})\\) se le conoce con el nombre de distribución posterior. Nótese que el denominador no depende del vector de parámetros y considerando a los datos observados como fijos, corresponde a una constante y puede ser obviada. Por lo tanto, otra representación de la regla de Bayes está dada por \\[ p(\\boldsymbol{\\theta} \\mid \\mathbf{Y})\\propto p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta}) \\] "],["inferencia-bayesiana..html", "4.2 Inferencia Bayesiana.", " 4.2 Inferencia Bayesiana. En términos de estimación, inferencia y predicción, el enfoque Bayesiano supone dos momentos o etapas: Antes de la recolección de las datos, en donde el investigador propone, basado en su conocimiento, experiencia o fuentes externas, una distribución de probabilidad previa para el parámetro de interés. Después de la recolección de los datos. Siguiendo el teorema de Bayes, el investigador actualiza su conocimiento acerca del comportamiento probabilístico del parámetro de interés mediante la distribución posterior de este. "],["modelos-uniparamétricos.html", "4.3 Modelos uniparamétricos", " 4.3 Modelos uniparamétricos Los modelos que están definidos en términos de un solo parámetro que pertenece al conjunto de los números reales se definen como modelos uniparamétricos. 4.3.1 Modelo de unidad: Bernoulli Suponga que \\(Y\\) es una variable aleatoria con distribución Bernoulli dada por: \\[ p(Y \\mid \\theta)=\\theta^y(1-\\theta)^{1-y}I_{\\{0,1\\}}(y) \\] Como el parámetro \\(\\theta\\) está restringido al espacio \\(\\Theta=[0,1]\\), entonces es posible formular varias opciones para la distribución previa del parámetro. En particular, la distribución uniforme restringida al intervalo \\([0,1]\\) o la distribución Beta parecen ser buenas opciones. Puesto que la distribución uniforme es un caso particular de la distribución Beta. Por lo tanto la distribución previa del parámetro \\(\\theta\\) estará dada por \\[ \\begin{equation} p(\\theta \\mid \\alpha,\\beta)= \\frac{1}{Beta(\\alpha,\\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}I_{[0,1]}(\\theta). \\end{equation} \\] y la distribución posterior del parámetro \\(\\theta\\) sigue una distribución \\[ \\begin{equation*} \\theta \\mid Y \\sim Beta(y+\\alpha,\\beta-y+1) \\end{equation*} \\] Cuando se tiene una muestra aleatoria \\(Y_1,\\ldots,Y_n\\) de variables con distribución Bernoulli de parámetro \\(\\theta\\), entonces la distribución posterior del parámetro de interés es \\[ \\begin{equation*} \\theta \\mid Y_1,\\ldots,Y_n \\sim Beta\\left(\\sum_{i=1}^ny_i+\\alpha,\\beta-\\sum_{i=1}^ny_i+n\\right) \\end{equation*} \\] Obejtivo Estimar la proporción de personas que están por debajo de la linea pobreza. Es decir, \\[ P = \\frac{\\sum_{U}y_i}{N} \\] donde \\(y_i\\) toma el valor de 1 cuando el ingreso de la persona es menor a la linea de pobreza 0 en caso contrario El estimador de \\(P\\) esta dado por: \\[ \\hat{P} = \\frac{\\sum_{s}w_{i}y_{i}}{\\sum_{s}{w_{i} }} \\] con \\(w_i\\) el factor de expansión para la \\(i-ésima\\) observación. Además, y obtener \\(\\widehat{Var}\\left(\\hat{P}\\right)\\). 4.3.1.1 Práctica en R library(tidyverse) encuesta &lt;- readRDS(&quot;Recursos/Día2/Sesion1/Data/encuestaDOM21N1.rds&quot;) Sea \\(Y\\) la variable aleatoria \\[ Y_{i}=\\begin{cases} 1 &amp; ingreso&lt;lp\\\\ 0 &amp; ingreso\\geq lp \\end{cases} \\] El tamaño de la muestra es de 6796 en la dam 1 datay &lt;- encuesta %&gt;% filter(dam_ee == 1) %&gt;% transmute(y = ifelse(ingcorte &lt; lp, 1,0)) addmargins(table(datay$y)) 0 1 Sum 5063 1733 6796 Un grupo de estadístico experto decide utilizar una distribución previa Beta, definiendo los parámetros de la distribución previa como \\(Beta(\\alpha=1, \\beta=1)\\). La distribución posterior del parámetro de interés, que representa la probabilidad de estar por debajo de la linea de pobreza, es \\(Beta(1733 + 1, 1 - 1733 + 6796)=Beta(1734, 5064)\\) Figura 4.1: Distribución previa (línea roja) y distribución posterior (línea negra) La estimación del parámetro estaría dado por: \\[ E(X) = \\frac{\\alpha}{\\alpha + \\beta} = \\frac{1734}{1734+ 5064} = 0.255075 \\] luego, el intervalo de credibilidad para la distribución posterior es. n = length(datay$y) n1 = sum(datay$y) qbeta(c(0.025, 0.975), shape1 = 1 + n1, shape2 = 1 - n1 + n) ## [1] 0.2447824 0.2655041 4.3.1.2 Práctica en STAN En STAN es posible obtener el mismo tipo de inferencia creando cuatro cadenas cuya distribución de probabilidad coincide con la distribución posterior del ejemplo. data { // Entrada el modelo int&lt;lower=0&gt; n; // Numero de observaciones int y[n]; // Vector de longitud n real a; real b; } parameters { // Definir parámetro real&lt;lower=0, upper=1&gt; theta; } model { // Definir modelo y ~ bernoulli(theta); theta ~ beta(a, b); // Distribución previa } generated quantities { real ypred[n]; // vector de longitud n for (ii in 1:n){ ypred[ii] = bernoulli_rng(theta); } } Para compilar STAN debemos definir los parámetros de entrada sample_data &lt;- list(n = nrow(datay), y = datay$y, a = 1, b = 1) Para ejecutar STAN en R tenemos la librería rstan library(rstan) Bernoulli &lt;- &quot;Recursos/Día2/Sesion1/Data/modelosStan/1Bernoulli.stan&quot; options(mc.cores = parallel::detectCores()) rstan::rstan_options(auto_write = TRUE) # speed up running time model_Bernoulli &lt;- stan( file = Bernoulli, # Stan program data = sample_data, # named list of data verbose = FALSE, warmup = 500, # number of warmup iterations per chain iter = 1000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) ) saveRDS(model_Bernoulli, file = &quot;Recursos/Día2/Sesion1/0Recursos/Bernoulli/model_Bernoulli.rds&quot;) model_Bernoulli &lt;- readRDS(&quot;Recursos/Día2/Sesion1/0Recursos/Bernoulli/model_Bernoulli.rds&quot;) La estimación del parámetro \\(\\theta\\) es: tabla_Ber1 &lt;- summary(model_Bernoulli, pars = &quot;theta&quot;)$summary tabla_Ber1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 0.255 2e-04 0.0053 0.2445 0.2515 0.255 0.2584 0.2657 703.8891 1.0011 Para observar las cadenas compilamos las lineas de código library(posterior) library(ggplot2) temp &lt;- as_draws_df(as.array(model_Bernoulli,pars = &quot;theta&quot;)) p1 &lt;- ggplot(data = temp, aes(x = theta))+ geom_density(color = &quot;blue&quot;, size = 2) + stat_function(fun = posterior1, args = list(y = datay$y), size = 2) + theme_bw(base_size = 20) + labs(x = latex2exp::TeX(&quot;\\\\theta&quot;), y = latex2exp::TeX(&quot;f(\\\\theta)&quot;)) #ggsave(filename = &quot;Recursos/Día2/Sesion1/0Recursos/Bernoulli/Bernoulli2.png&quot;,plot = p1) p1 Figura 4.2: Resultado con STAN (línea azul) y posterior teórica (línea negra) Para validar las cadenas library(bayesplot) library(patchwork) posterior_theta &lt;- as.array(model_Bernoulli, pars = &quot;theta&quot;) (mcmc_dens_chains(posterior_theta) + mcmc_areas(posterior_theta) ) / traceplot(model_Bernoulli, pars = &quot;theta&quot;, inc_warmup = T) Predicción de \\(Y\\) en cada una de las iteraciones de las cadenas. y_pred_B &lt;- as.array(model_Bernoulli, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, 1:n] ppc_dens_overlay(y = datay$y, y_pred2) 4.3.2 Modelo de área: Binomial Cuando se dispone de una muestra aleatoria de variables con distribución Bernoulli \\(Y_1,\\ldots,Y_n\\), la inferencia Bayesiana se puede llevar a cabo usando la distribución Binomial, puesto que es bien sabido que la suma de variables aleatorias Bernoulli \\[ \\begin{equation*} S=\\sum_{i=1}^nY_i \\end{equation*} \\] sigue una distribución Binomial. Es decir: \\[ \\begin{equation} p(S \\mid \\theta)=\\binom{n}{s}\\theta^s(1-\\theta)^{n-s}I_{\\{0,1,\\ldots,n\\}}(s), \\end{equation} \\] Nótese que la distribución Binomial es un caso general para la distribución Bernoulli, cuando \\(n=1\\). Por lo tanto es natural suponer que distribución previa del parámetro \\(\\theta\\) estará dada por \\[ \\begin{equation} p(\\theta \\mid \\alpha,\\beta)= \\frac{1}{Beta(\\alpha,\\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}I_{[0,1]}(\\theta). \\end{equation} \\] La distribución posterior del parámetro \\(\\theta\\) sigue una distribución \\[ \\begin{equation*} \\theta \\mid S \\sim Beta(s+\\alpha,\\beta-s+n) \\end{equation*} \\] Ahora, cuando se tiene una sucesión de variables aleatorias \\(S_1,\\ldots,S_d, \\ldots,S_D\\) independientes y con distribución \\(Binomial(n_d,\\theta_d)\\) para \\(d=1,\\ldots,K\\), entonces la distribución posterior del parámetro de interés \\(\\theta_d\\) es \\[ \\begin{equation*} \\theta_d \\mid s_d \\sim Beta\\left(s_d+\\alpha,\\ \\beta+ n_d- s_d\\right) \\end{equation*} \\] Obejtivo Estimar la proporción de personas que están por debajo de la linea pobreza en el \\(d-ésimo\\) dominio. Es decir, \\[ P_d = \\frac{\\sum_{U_d}y_{di}}{N_d} \\] donde \\(y_i\\) toma el valor de 1 cuando el ingreso de la persona es menor a la linea de pobreza 0 en caso contrario. El estimador de \\(P\\) esta dado por: \\[ \\hat{P_d} = \\frac{\\sum_{s_d}w_{di}y_{di}}{\\sum_{s_d}{w_{di} }} \\] con \\(w_{di}\\) el factor de expansión para la \\(i-ésima\\) observación en el \\(d-ésimo\\) dominio. 4.3.2.1 Práctica en STAN Sea \\(S_k\\) el conteo de personas en condición de pobreza en el \\(k-ésimo\\) departamento en la muestra. dataS &lt;- encuesta %&gt;% transmute( dam = dam_ee, y = ifelse(ingcorte &lt; lp, 1,0) ) %&gt;% group_by(dam) %&gt;% summarise(nd = n(), #Número de ensayos Sd = sum(y) #Número de éxito ) tba(dataS) dam nd Sd 1 6796 1733 2 1556 397 3 2013 979 4 2912 1093 5 466 125 6 1649 295 7 821 365 8 1199 268 9 2012 235 10 1200 561 11 2678 432 12 2543 744 13 2534 416 14 780 195 15 773 96 16 466 203 17 1059 214 18 2907 381 19 460 9 20 669 162 21 3381 1012 22 2318 625 23 2260 368 24 909 138 25 9011 1840 26 401 55 27 963 171 28 923 157 29 1619 675 30 478 84 31 346 80 32 17969 4554 Creando código de STAN data { int&lt;lower=0&gt; K; // Número de provincia int&lt;lower=0&gt; n[K]; // Número de ensayos int&lt;lower=0&gt; s[K]; // Número de éxitos real a; real b; } parameters { real&lt;lower=0, upper=1&gt; theta[K]; // theta_d|s_d } model { for(kk in 1:K) { s[kk] ~ binomial(n[kk], theta[kk]); } to_vector(theta) ~ beta(a, b); } generated quantities { real spred[K]; // vector de longitud K for(kk in 1:K){ spred[kk] = binomial_rng(n[kk],theta[kk]); } } Preparando el código de STAN Binomial2 &lt;- &quot;Recursos/Día2/Sesion1/Data/modelosStan/3Binomial.stan&quot; Organizando datos para STAN sample_data &lt;- list(K = nrow(dataS), s = dataS$Sd, n = dataS$nd, a = 1, b = 1) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) model_Binomial2 &lt;- stan( file = Binomial2, # Stan program data = sample_data, # named list of data verbose = FALSE, warmup = 500, # number of warmup iterations per chain iter = 1000, # total number of iterations per chain cores = 4, # number of cores (could use one per chain) ) saveRDS(model_Binomial2, &quot;Recursos/Día2/Sesion1/0Recursos/Binomial/model_Binomial2.rds&quot;) model_Binomial2 &lt;- readRDS(&quot;Recursos/Día2/Sesion1/0Recursos/Binomial/model_Binomial2.rds&quot;) La estimación del parámetro \\(\\theta\\) es: tabla_Bin1 &lt;-summary(model_Binomial2, pars = &quot;theta&quot;)$summary tabla_Bin1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta[1] 0.2550 1e-04 0.0054 0.2443 0.2514 0.2549 0.2586 0.2663 5541.426 0.9984 theta[2] 0.2557 2e-04 0.0111 0.2346 0.2480 0.2555 0.2633 0.2775 4685.553 0.9988 theta[3] 0.4863 2e-04 0.0112 0.4639 0.4790 0.4863 0.4937 0.5081 4275.709 0.9995 theta[4] 0.3754 1e-04 0.0090 0.3578 0.3692 0.3756 0.3815 0.3930 4356.266 0.9993 theta[5] 0.2688 3e-04 0.0210 0.2310 0.2543 0.2682 0.2831 0.3099 5505.855 0.9987 theta[6] 0.1792 1e-04 0.0095 0.1612 0.1729 0.1790 0.1858 0.1983 5539.281 0.9985 theta[7] 0.4447 3e-04 0.0172 0.4108 0.4336 0.4445 0.4562 0.4795 4289.643 0.9989 theta[8] 0.2240 2e-04 0.0115 0.2019 0.2161 0.2238 0.2315 0.2470 4261.208 0.9993 theta[9] 0.1172 1e-04 0.0077 0.1023 0.1121 0.1170 0.1222 0.1328 3835.345 0.9994 theta[10] 0.4673 2e-04 0.0142 0.4402 0.4577 0.4671 0.4769 0.4949 5121.700 0.9992 theta[11] 0.1617 1e-04 0.0071 0.1483 0.1568 0.1614 0.1667 0.1760 5501.146 0.9983 theta[12] 0.2930 1e-04 0.0089 0.2759 0.2870 0.2928 0.2988 0.3105 4834.824 0.9987 theta[13] 0.1644 1e-04 0.0073 0.1503 0.1595 0.1643 0.1691 0.1788 5452.961 0.9981 theta[14] 0.2505 2e-04 0.0148 0.2216 0.2404 0.2505 0.2598 0.2813 3949.950 0.9985 theta[15] 0.1253 2e-04 0.0120 0.1024 0.1170 0.1250 0.1334 0.1484 3561.371 0.9985 theta[16] 0.4359 3e-04 0.0228 0.3920 0.4206 0.4356 0.4507 0.4811 4413.229 0.9989 theta[17] 0.2030 2e-04 0.0122 0.1803 0.1949 0.2024 0.2106 0.2285 3553.277 0.9984 theta[18] 0.1313 1e-04 0.0061 0.1193 0.1272 0.1313 0.1355 0.1428 3265.019 0.9987 theta[19] 0.0217 1e-04 0.0068 0.0105 0.0168 0.0211 0.0258 0.0367 4589.526 0.9989 theta[20] 0.2427 3e-04 0.0163 0.2119 0.2310 0.2426 0.2539 0.2754 3692.533 0.9985 theta[21] 0.2995 1e-04 0.0081 0.2833 0.2941 0.2995 0.3047 0.3152 4654.262 0.9994 theta[22] 0.2701 1e-04 0.0091 0.2520 0.2642 0.2701 0.2759 0.2884 4987.638 0.9986 theta[23] 0.1630 1e-04 0.0080 0.1477 0.1573 0.1628 0.1684 0.1788 3848.725 0.9985 theta[24] 0.1524 2e-04 0.0114 0.1310 0.1442 0.1522 0.1603 0.1752 3602.961 0.9989 theta[25] 0.2042 1e-04 0.0041 0.1964 0.2014 0.2041 0.2070 0.2122 4184.041 0.9995 theta[26] 0.1388 3e-04 0.0177 0.1069 0.1265 0.1381 0.1505 0.1760 4957.831 0.9985 theta[27] 0.1783 2e-04 0.0118 0.1560 0.1700 0.1783 0.1863 0.2028 4550.942 0.9987 theta[28] 0.1706 2e-04 0.0125 0.1467 0.1618 0.1705 0.1796 0.1950 4629.408 0.9993 theta[29] 0.4171 2e-04 0.0126 0.3926 0.4084 0.4168 0.4257 0.4418 5023.648 0.9989 theta[30] 0.1772 3e-04 0.0176 0.1442 0.1651 0.1764 0.1884 0.2137 4571.020 0.9988 theta[31] 0.2328 4e-04 0.0227 0.1895 0.2177 0.2324 0.2477 0.2811 4051.155 0.9993 theta[32] 0.2535 1e-04 0.0032 0.2473 0.2514 0.2535 0.2557 0.2597 3530.149 0.9992 Para validar las cadenas (s1 &lt;- mcmc_areas(as.array(model_Binomial2, pars = &quot;theta&quot;))) # ggsave(filename = &quot;Recursos/Día2/Sesion1/0Recursos/Binomial/Binomial1.png&quot;,plot = s1) (s2 &lt;- mcmc_trace(as.array(model_Binomial2, pars = &quot;theta&quot;))) # ggsave(filename = &quot;Recursos/Día2/Sesion1/0Recursos/Binomial/Binomial2.png&quot;, # plot = s2,width = 20, height = 20, units = &quot;cm&quot;) y_pred_B &lt;- as.array(model_Binomial2, pars = &quot;spred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 200) y_pred2 &lt;- y_pred_B[rowsrandom, ] g1 &lt;- ggplot(data = dataS, aes(x = Sd))+ geom_histogram(aes(y = ..density..)) + geom_density(size = 2, color = &quot;blue&quot;) + labs(y = &quot;&quot;)+ theme_bw(10) + yaxis_title(FALSE) + xaxis_title(FALSE) + yaxis_text(FALSE) + yaxis_ticks(FALSE) g2 &lt;- ppc_dens_overlay(y = dataS$Sd, y_pred2) g1/g2 4.3.3 Modelo de unidad: Normal con media desconocida Suponga que \\(Y_1,\\cdots,Y_n\\) son variables independientes e idénticamente distribuidos con distribución \\(Normal(\\theta,\\sigma^2)\\) con \\(\\theta\\) desconocido pero \\(\\sigma^2\\) conocido. De esta forma, la función de verosimilitud de los datos está dada por \\[ \\begin{align*} p(\\mathbf{Y} \\mid \\theta) &amp;=\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{1}{2\\sigma^2}(y_i-\\theta)^2\\right\\}I_\\mathbb{R}(y) \\\\ &amp;=(2\\pi\\sigma^2)^{-n/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\theta)^2\\right\\} \\end{align*} \\] Como el parámetro \\(\\theta\\) puede tomar cualquier valor en los reales, es posible asignarle una distribución previa \\(\\theta \\sim Normal(\\mu,\\tau^2)\\). Bajo este marco de referencia se tienen los siguientes resultados La distribución posterior del parámetro de interés \\(\\theta\\) sigue una distribución \\[ \\begin{equation*} \\theta|\\mathbf{Y} \\sim Normal(\\mu_n,\\tau^2_n) \\end{equation*} \\] En donde \\[ \\begin{equation} \\mu_n=\\frac{\\frac{n}{\\sigma^2}\\bar{Y}+\\frac{1}{\\tau^2}\\mu}{\\frac{n}{\\sigma^2}+\\frac{1}{\\tau^2}} \\ \\ \\ \\ \\ \\ \\ \\text{y} \\ \\ \\ \\ \\ \\ \\ \\tau_n^2=\\left(\\frac{n}{\\sigma^2}+\\frac{1}{\\tau^2}\\right)^{-1} \\end{equation} \\] Obejtivo Estimar el ingreso medio de la población, es decir, \\[ \\bar{Y} = \\frac{\\sum_Uy_i}{N} \\] donde, \\(y_i\\) es el ingreso de las personas. El estimador de \\(\\bar{Y}\\) esta dado por \\[ \\hat{\\bar{Y}} = \\frac{\\sum_{s}w_{i}y_{i}}{\\sum_s{w_i}} \\] y obtener \\(\\widehat{Var}\\left(\\hat{\\bar{Y}}\\right)\\). 4.3.3.1 Práctica en STAN Sea \\(Y\\) el logaritmo del ingreso dataNormal &lt;- encuesta %&gt;% transmute( dam_ee , logIngreso = log(ingcorte +1)) %&gt;% filter(dam_ee == 1) #3 media &lt;- mean(dataNormal$logIngreso) Sd &lt;- sd(dataNormal$logIngreso) g1 &lt;- ggplot(dataNormal,aes(x = logIngreso))+ geom_density(size = 2, color = &quot;blue&quot;) + stat_function(fun =dnorm, args = list(mean = media, sd = Sd), size =2) + theme_bw(base_size = 20) + labs(y = &quot;&quot;, x = (&quot;Log(Ingreso)&quot;)) g2 &lt;- ggplot(dataNormal, aes(sample = logIngreso)) + stat_qq() + stat_qq_line() + theme_bw(base_size = 20) g1|g2 Creando código de STAN data { int&lt;lower=0&gt; n; // Número de observaciones real y[n]; // LogIngreso real &lt;lower=0&gt; Sigma; // Desviación estándar } parameters { real theta; } model { y ~ normal(theta, Sigma); theta ~ normal(0, 1000); // Distribución previa } generated quantities { real ypred[n]; // Vector de longitud n for(kk in 1:n){ ypred[kk] = normal_rng(theta,Sigma); } } Preparando el código de STAN NormalMedia &lt;- &quot;Recursos/Día2/Sesion1/Data/modelosStan/4NormalMedia.stan&quot; Organizando datos para STAN sample_data &lt;- list(n = nrow(dataNormal), Sigma = sd(dataNormal$logIngreso), y = dataNormal$logIngreso) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) model_NormalMedia &lt;- stan( file = NormalMedia, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(model_NormalMedia, &quot;Recursos/Día2/Sesion1/0Recursos/Normal/model_NormalMedia.rds&quot;) model_NormalMedia &lt;- readRDS(&quot;Recursos/Día2/Sesion1/0Recursos/Normal/model_NormalMedia.rds&quot;) La estimación del parámetro \\(\\theta\\) es: tabla_Nor1 &lt;- summary(model_NormalMedia, pars = &quot;theta&quot;)$summary tabla_Nor1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 9.0982 4e-04 0.0095 9.0786 9.0917 9.0985 9.1046 9.1167 654.8346 1.0008 posterior_theta &lt;- as.array(model_NormalMedia, pars = &quot;theta&quot;) (mcmc_dens_chains(posterior_theta) + mcmc_areas(posterior_theta) ) / mcmc_trace(posterior_theta) y_pred_B &lt;- as.array(model_NormalMedia, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] ppc_dens_overlay(y = as.numeric(dataNormal$logIngreso), y_pred2)/ ppc_dens_overlay(y = exp(as.numeric(dataNormal$logIngreso))-1, exp(y_pred2)-1) + xlim(0,240000) "],["modelos-multiparamétricos.html", "4.4 Modelos multiparamétricos", " 4.4 Modelos multiparamétricos La distribución normal univariada que tiene dos parámetros: la media \\(\\theta\\) y la varianza \\(\\sigma^2\\). La distribución multinomial cuyo parámetro es un vector de probabilidades \\(\\boldsymbol{\\theta}\\). 4.4.1 Modelo de unidad: Normal con media y varianza desconocida Supongamos que se dispone de realizaciones de un conjunto de variables independientes e idénticamente distribuidas \\(Y_1,\\cdots,Y_n\\sim N(\\theta,\\sigma^2)\\). Cuando se desconoce tanto la media como la varianza de la distribución es necesario plantear diversos enfoques y situarse en el más conveniente, según el contexto del problema. En términos de la asignación de las distribuciones previas para \\(\\theta\\) y \\(\\sigma^2\\) es posible: Suponer que la distribución previa \\(p(\\theta)\\) es independiente de la distribución previa \\(p(\\sigma^2)\\) y que ambas distribuciones son informativas. Suponer que la distribución previa \\(p(\\theta)\\) es independiente de la distribución previa \\(p(\\sigma^2)\\) y que ambas distribuciones son no informativas. Suponer que la distribución previa para \\(\\theta\\) depende de \\(\\sigma^2\\) y escribirla como \\(p(\\theta \\mid \\sigma^2)\\), mientras que la distribución previa de \\(\\sigma^2\\) no depende de \\(\\theta\\) y se puede escribir como \\(p(\\sigma^2)\\). La distribución previa para el parámetro \\(\\theta\\) será \\[ \\begin{equation*} \\theta \\sim Normal(0,10000) \\end{equation*} \\] Y la distribución previa para el parámetro \\(\\sigma^2\\) será \\[ \\begin{equation*} \\sigma^2 \\sim IG(0.0001,0.0001) \\end{equation*} \\] La distribución posterior condicional de \\(\\theta\\) es \\[ \\begin{equation} \\theta \\mid \\sigma^2,\\mathbf{Y} \\sim Normal(\\mu_n,\\tau_n^2) \\end{equation} \\] En donde las expresiones para \\(\\mu_n\\) y \\(\\tau_n^2\\) están dados previamente. En el siguiente enlace enconará el libro: Modelos Bayesianos con R y STAN donde puede profundizar en el desarrollo matemático de los resultados anteriores. Obejtivo Estimar el ingreso medio de las personas, es decir, \\[ \\bar{Y} = \\frac{\\sum_Uy_i}{N} \\] donde, \\(y_i\\) es el ingreso de las personas. El estimador de \\(\\bar{Y}\\) esta dado por \\[ \\hat{\\bar{Y}} = \\frac{\\sum_{s}w_{i}y_{i}}{\\sum_s{w_i}} \\] y obtener \\(\\widehat{Var}\\left(\\hat{\\bar{Y}}\\right)\\). 4.4.1.1 Práctica en STAN Sea \\(Y\\) el logaritmo del ingreso dataNormal &lt;- encuesta %&gt;% transmute(dam_ee, logIngreso = log(ingcorte +1)) %&gt;% filter(dam_ee == 1) Creando código de STAN data { int&lt;lower=0&gt; n; real y[n]; } parameters { real sigma; real theta; } transformed parameters { real sigma2; sigma2 = pow(sigma, 2); } model { y ~ normal(theta, sigma); theta ~ normal(0, 1000); sigma2 ~ inv_gamma(0.001, 0.001); } generated quantities { real ypred[n]; // vector de longitud n for(kk in 1:n){ ypred[kk] = normal_rng(theta,sigma); } } Preparando el código de STAN NormalMeanVar &lt;- &quot;Recursos/Día2/Sesion1/Data/modelosStan/5NormalMeanVar.stan&quot; Organizando datos para STAN sample_data &lt;- list(n = nrow(dataNormal), y = dataNormal$logIngreso) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) model_NormalMedia &lt;- stan( file = NormalMeanVar, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(model_NormalMedia,&quot;Recursos/Día2/Sesion1/0Recursos/Normal/model_NormalMedia2.rds&quot;) model_NormalMedia &lt;- readRDS(&quot;Recursos/Día2/Sesion1/0Recursos/Normal/model_NormalMedia2.rds&quot;) La estimación del parámetro \\(\\theta\\) y \\(\\sigma^2\\) es: tabla_Nor2 &lt;- summary(model_NormalMedia, pars = c(&quot;theta&quot;, &quot;sigma2&quot;, &quot;sigma&quot;))$summary tabla_Nor2 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 9.0991 3e-04 0.0099 9.0801 9.0924 9.0989 9.1057 9.1189 1569.114 1.0002 sigma2 0.6318 2e-04 0.0108 0.6101 0.6244 0.6314 0.6392 0.6531 1935.498 0.9991 sigma 0.7948 2e-04 0.0068 0.7811 0.7902 0.7946 0.7995 0.8081 1935.196 0.9991 posterior_theta &lt;- as.array(model_NormalMedia, pars = &quot;theta&quot;) (mcmc_dens_chains(posterior_theta) + mcmc_areas(posterior_theta) ) / mcmc_trace(posterior_theta) posterior_sigma2 &lt;- as.array(model_NormalMedia, pars = &quot;sigma2&quot;) (mcmc_dens_chains(posterior_sigma2) + mcmc_areas(posterior_sigma2) ) / mcmc_trace(posterior_sigma2) posterior_sigma &lt;- as.array(model_NormalMedia, pars = &quot;sigma&quot;) (mcmc_dens_chains(posterior_sigma) + mcmc_areas(posterior_sigma) ) / mcmc_trace(posterior_sigma) y_pred_B &lt;- as.array(model_NormalMedia, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] ppc_dens_overlay(y = as.numeric(exp(dataNormal$logIngreso)-1), y_pred2) + xlim(0,240000) 4.4.2 Modelo Multinomial En esta sección discutimos el modelamiento bayesiano de datos provenientes de una distribución multinomial que corresponde a una extensión multivariada de la distribución binomial. Suponga que \\(\\textbf{Y}=(Y_1,\\ldots,Y_p)&#39;\\) es un vector aleatorio con distribución multinomial, así, su distribución está parametrizada por el vector \\(\\boldsymbol{\\theta}=(\\theta_1,\\ldots,\\theta_p)&#39;\\) y está dada por la siguiente expresión \\[ \\begin{equation} p(\\mathbf{Y} \\mid \\boldsymbol{\\theta})=\\binom{n}{y_1,\\ldots,y_p}\\prod_{i=1}^p\\theta_i^{y_i} \\ \\ \\ \\ \\ \\theta_i&gt;0 \\texttt{ , } \\sum_{i=1}^py_i=n \\texttt{ y } \\sum_{i=1}^p\\theta_i=1 \\end{equation} \\] Donde \\[ \\begin{equation*} \\binom{n}{y_1,\\ldots,y_p}=\\frac{n!}{y_1!\\cdots y_p!}. \\end{equation*} \\] Como cada parámetro \\(\\theta_i\\) está restringido al espacio \\(\\Theta=[0,1]\\), entonces es posible asignar a la distribución de Dirichlet como la distribución previa del vector de parámetros. Por lo tanto la distribución previa del vector de parámetros \\(\\boldsymbol{\\theta}\\), parametrizada por el vector de hiperparámetros \\(\\boldsymbol{\\alpha}=(\\alpha_1,\\ldots,\\alpha_p)&#39;\\), está dada por \\[ \\begin{equation} p(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\alpha})=\\frac{\\Gamma(\\alpha_1+\\cdots+\\alpha_p)}{\\Gamma(\\alpha_1)\\cdots\\Gamma(\\alpha_p)} \\prod_{i=1}^p\\theta_i^{\\alpha_i-1} \\ \\ \\ \\ \\ \\alpha_i&gt;0 \\texttt{ y } \\sum_{i=1}^p\\theta_i=1 \\end{equation} \\] La distribución posterior del parámetro \\(\\boldsymbol{\\theta}\\) sigue una distribución \\(Dirichlet(y_1+\\alpha_1,\\ldots,y_p+\\alpha_p)\\) Obejtivo Sea \\(N_1\\) el número de personas ocupadas, \\(N_2\\) Número de personas desocupadas, \\(N_3\\) es el número de personas inactivas en la población y \\(N = N_1 +N_2 + N_3\\). Entonces el objetivo es estimar el vector de parámetros \\(\\boldsymbol{P}=\\left(P_{1},P_{2},P_{3}\\right)\\), con \\(P_{k}=\\frac{N_{k}}{N}\\), para \\(k=1,2,3\\), El estimador de \\(\\boldsymbol{P}\\) esta dado por \\[ \\hat{\\boldsymbol{P}} =\\left(\\hat{P}_{1},\\hat{P}_{2},\\hat{P}_{3}\\right) \\] donde, \\[ \\hat{P}_{k} = \\frac{\\sum_{s}w_{i}y_{ik}}{\\sum_s{w_i}} = \\frac{\\hat{N}_k}{\\hat{N}} \\] y \\(y_{ik}\\) toma el valor 1 cuando la \\(i-ésima\\) persona responde la \\(k-ésima\\) categoría (Ocupado o Desocupado o Inactivo). Además, obtener \\(\\widehat{Var}\\left(\\hat{P}_{k}\\right)\\). 4.4.2.1 Práctica en STAN Sea \\(Y\\) condición de actividad laboral dataMult &lt;- encuesta %&gt;% filter(condact3 %in% 1:3) %&gt;% transmute( empleo = as_factor(condact3)) %&gt;% group_by(empleo) %&gt;% tally() %&gt;% mutate(theta = n/sum(n)) tba(dataMult) empleo n theta Ocupado 33047 0.5267 Desocupado 2371 0.0378 Inactivo 27324 0.4355 donde 1 corresponde a Ocupado, 2 son los Desocupado y 3 son Inactivo Creando código de STAN data { int&lt;lower=0&gt; k; // Número de cátegoria int y[k]; // Número de exitos vector[k] alpha; // Parámetro de las distribción previa } parameters { simplex[k] theta; } transformed parameters { real delta; // Tasa de desocupación delta = theta[2]/ (theta[2] + theta[1]); // (Desocupado)/(Desocupado + Ocupado) } model { y ~ multinomial(theta); theta ~ dirichlet(alpha); } generated quantities { int ypred[k]; ypred = multinomial_rng(theta, sum(y)); } Preparando el código de STAN Multinom &lt;- &quot;Recursos/Día2/Sesion1/Data/modelosStan/6Multinom.stan&quot; Organizando datos para STAN sample_data &lt;- list(k = nrow(dataMult), y = dataMult$n, alpha = c(0.5, 0.5, 0.5)) Para ejecutar STAN en R tenemos la librería rstan options(mc.cores = parallel::detectCores()) model_Multinom &lt;- stan( file = Multinom, data = sample_data, verbose = FALSE, warmup = 500, iter = 1000, cores = 4 ) saveRDS(model_Multinom, &quot;Recursos/Día2/Sesion1/0Recursos/Multinomial/model_Multinom.rds&quot;) model_Multinom &lt;- readRDS(&quot;Recursos/Día2/Sesion1/0Recursos/Multinomial/model_Multinom.rds&quot;) La estimación del parámetro \\(\\theta\\) y \\(\\delta\\) es: tabla_Mul1 &lt;- summary(model_Multinom, pars = c(&quot;delta&quot;, &quot;theta&quot;))$summary tabla_Mul1 %&gt;% tba() mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat delta 0.0670 0 0.0013 0.0644 0.0661 0.0670 0.0679 0.0695 1414.051 1.0014 theta[1] 0.5268 0 0.0019 0.5231 0.5255 0.5268 0.5280 0.5305 1692.834 0.9989 theta[2] 0.0378 0 0.0008 0.0363 0.0373 0.0378 0.0383 0.0393 1379.738 1.0018 theta[3] 0.4354 0 0.0019 0.4317 0.4341 0.4354 0.4367 0.4392 1633.676 0.9997 posterior_theta1 &lt;- as.array(model_Multinom, pars = &quot;theta[1]&quot;) (mcmc_dens_chains(posterior_theta1) + mcmc_areas(posterior_theta1) ) / mcmc_trace(posterior_theta1) posterior_theta2 &lt;- as.array(model_Multinom, pars = &quot;theta[2]&quot;) (mcmc_dens_chains(posterior_theta2) + mcmc_areas(posterior_theta2) ) / mcmc_trace(posterior_theta2) posterior_theta3 &lt;- as.array(model_Multinom, pars = &quot;theta[3]&quot;) (mcmc_dens_chains(posterior_theta3) + mcmc_areas(posterior_theta3) ) / mcmc_trace(posterior_theta3) posterior_delta &lt;- as.array(model_Multinom, pars = &quot;delta&quot;) (mcmc_dens_chains(posterior_delta) + mcmc_areas(posterior_delta) ) / mcmc_trace(posterior_delta) La imagen es muy pesada no se carga al repositorio. n &lt;- nrow(dataMult) y_pred_B &lt;- as.array(model_Multinom, pars = &quot;ypred&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, 1:n] ppc_dens_overlay(y = as.numeric(dataMult$n), y_pred2) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
